**Normalization Strategy:**  
used batch normalization, so a new scaler is used for every batch during training

**Learning Rate:** 0.001

**Batch Size:** 20

**Activation functions:** used tanh, sigmoid and ReLu
